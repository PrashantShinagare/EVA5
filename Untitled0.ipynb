{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjugtNKMx1ZKgOoQXFaZc2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrashantShinagare/EVA5/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1ZZX8FrenSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# For licensing see accompanying LICENSE file.\n",
        "# Copyright (C) 2019 Apple Inc. All Rights Reserved.\n",
        "#\n",
        "\n",
        "import csv\n",
        "import copy\n",
        "import time\n",
        "import inspect\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from itertools import chain\n",
        "from inspect import signature\n",
        "from functools import lru_cache as cache\n",
        "from collections import defaultdict, namedtuple\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTjXbErofa21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################\n",
        "# Constants\n",
        "####################\n",
        "\n",
        "CIFAR10_MEAN, CIFAR10_STD = [\n",
        "    (125.31, 122.95, 113.87),  # equals np.mean(cifar10()['train']['data'], axis=(0,1,2))\n",
        "    (62.99, 62.09, 66.70),  # equals np.std(cifar10()['train']['data'], axis=(0,1,2))\n",
        "]\n",
        "\n",
        "CIFAR10_CLASSES = 'airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck'.split(', ')\n",
        "\n",
        "\n",
        "#####################\n",
        "# dict utils\n",
        "#####################\n",
        "\n",
        "def union(*dicts):\n",
        "    return {k: v for d in dicts for (k, v) in d.items()}\n",
        "\n",
        "\n",
        "def make_tuple(path):\n",
        "    return (path,) if isinstance(path, str) else path\n",
        "\n",
        "\n",
        "def map_values(func, dct):\n",
        "    return {k: func(v) for k, v in dct.items()}\n",
        "\n",
        "\n",
        "def path_iter(nested_dict, pfx=()):\n",
        "    for name, val in nested_dict.items():\n",
        "        if isinstance(val, dict):\n",
        "            yield from path_iter(val, pfx + make_tuple(name))\n",
        "        else:\n",
        "            yield (pfx + make_tuple(name), val)\n",
        "\n",
        "\n",
        "def map_nested(func, nested_dict):\n",
        "    return {k: map_nested(func, v) if isinstance(v, dict) else func(v) for k, v in nested_dict.items()}\n",
        "\n",
        "\n",
        "def group_by_key(seq):\n",
        "    res = defaultdict(list)\n",
        "    for k, v in seq:\n",
        "        res[k].append(v)\n",
        "    return res\n",
        "\n",
        "\n",
        "def reorder(dct, keys):\n",
        "    return {k: dct[k] for k in keys}\n",
        "\n",
        "\n",
        "def mean_members(dictionary):\n",
        "    \"\"\"Returns a dictionary with the same keys and the mean of the values\"\"\"\n",
        "    return {k: np.mean(v) for k, v in dictionary.items()}\n",
        "\n",
        "\n",
        "def identity(value):\n",
        "    return value\n",
        "\n",
        "\n",
        "def map_types(mapping, net):\n",
        "    def f(node):\n",
        "        typ, *rest = node\n",
        "        return (mapping.get(typ, typ), *rest)\n",
        "\n",
        "    return map_nested(f, net)\n",
        "\n",
        "\n",
        "def to(*args, **kwargs):\n",
        "    \"\"\"\n",
        "    Returns a closure that applies x.to(*args, **kwargs) to x\n",
        "    \"\"\"\n",
        "    def apply_to(x):\n",
        "        return x.to(*args, **kwargs)\n",
        "    return apply_to\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bjjNP6jf0II",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################\n",
        "# graph building\n",
        "#####################\n",
        "\n",
        "\n",
        "def build_graph(net, path_map='_'.join):\n",
        "    net = {path: node if len(node) is 3 else (*node, None) for path, node in path_iter(net)}\n",
        "    default_inputs = chain([('input',)], net.keys())\n",
        "    resolve_path = lambda path, pfx: pfx + path if (pfx + path in net or not pfx) else resolve_path(net, path, pfx[:-1])\n",
        "    return {path_map(path): (typ, value, (\n",
        "        [path_map(default)] if inputs is None else [path_map(resolve_path(make_tuple(k), path[:-1])) for k in inputs]))\n",
        "            for (path, (typ, value, inputs)), default in zip(net.items(), default_inputs)}\n",
        "\n",
        "\n",
        "# node definitions\n",
        "empty_signature = inspect.Signature()\n",
        "\n",
        "\n",
        "class node_def(namedtuple('node_def', ['type'])):\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return (self.type, dict(signature(self.type).bind(*args, **kwargs).arguments))\n",
        "\n",
        "\n",
        "#####################\n",
        "# Layers\n",
        "#####################\n",
        "\n",
        "class Add(namedtuple('Add', [])):\n",
        "    def __call__(self, x, y): return x + y\n",
        "\n",
        "\n",
        "class AddWeighted(namedtuple('AddWeighted', ['wx', 'wy'])):\n",
        "    def __call__(self, x, y): return self.wx * x + self.wy * y\n",
        "\n",
        "\n",
        "class Identity(namedtuple('Identity', [])):\n",
        "    def __call__(self, x): return x\n",
        "\n",
        "\n",
        "class BatchNorm(nn.BatchNorm2d):\n",
        "    def __init__(self, num_features, eps=1e-05, momentum=0.1, weight=True, bias=True):\n",
        "        super().__init__(num_features, eps=eps, momentum=momentum)\n",
        "        self.weight.data.fill_(1.0)\n",
        "        self.bias.data.fill_(0.0)\n",
        "        self.weight.requires_grad = weight\n",
        "        self.bias.requires_grad = bias\n",
        "\n",
        "\n",
        "class GhostBatchNorm(BatchNorm):\n",
        "    def __init__(self, num_features, num_splits, **kw):\n",
        "        super().__init__(num_features, **kw)\n",
        "        self.num_splits = num_splits\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features * self.num_splits))\n",
        "        self.register_buffer('running_var', torch.ones(num_features * self.num_splits))\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        if (self.training is True) and (mode is False):  # lazily collate stats when we are going to use them\n",
        "            self.running_mean = torch.mean(self.running_mean.view(self.num_splits, self.num_features), dim=0).repeat(\n",
        "                self.num_splits)\n",
        "            self.running_var = torch.mean(self.running_var.view(self.num_splits, self.num_features), dim=0).repeat(\n",
        "                self.num_splits)\n",
        "        return super().train(mode)\n",
        "\n",
        "    def forward(self, input):\n",
        "        N, C, H, W = input.shape\n",
        "        if self.training or not self.track_running_stats:\n",
        "            return F.batch_norm(\n",
        "                input.view(-1, C * self.num_splits, H, W), self.running_mean, self.running_var,\n",
        "                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
        "                True, self.momentum, self.eps).view(N, C, H, W)\n",
        "        else:\n",
        "            return F.batch_norm(\n",
        "                input, self.running_mean[:self.num_features], self.running_var[:self.num_features],\n",
        "                self.weight, self.bias, False, self.momentum, self.eps)\n",
        "\n",
        "\n",
        "class Mul(nn.Module):\n",
        "    def __init__(self, weight):\n",
        "        super().__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return x * self.weight\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), x.size(1))\n",
        "\n",
        "\n",
        "# Losses\n",
        "class CrossEntropyLoss(namedtuple('CrossEntropyLoss', [])):\n",
        "    def __call__(self, log_probs, target):\n",
        "        return torch.nn.functional.nll_loss(log_probs, target, reduction='none')\n",
        "\n",
        "\n",
        "class KLLoss(namedtuple('KLLoss', [])):\n",
        "    def __call__(self, log_probs):\n",
        "        return -log_probs.mean(dim=1)\n",
        "\n",
        "\n",
        "class Correct(namedtuple('Correct', [])):\n",
        "    def __call__(self, classifier, target):\n",
        "        return classifier.max(dim=1)[1] == target\n",
        "\n",
        "\n",
        "class LogSoftmax(namedtuple('LogSoftmax', ['dim'])):\n",
        "    def __call__(self, x):\n",
        "        return torch.nn.functional.log_softmax(x, self.dim, _stacklevel=5)\n",
        "\n",
        "\n",
        "conv = node_def(nn.Conv2d)\n",
        "linear = node_def(nn.Linear)\n",
        "batch_norm = node_def(BatchNorm)\n",
        "pool = node_def(nn.MaxPool2d)\n",
        "relu = node_def(nn.ReLU)\n",
        "\n",
        "\n",
        "def conv_block(c_in, c_out):\n",
        "    return {\n",
        "        'conv': conv(\n",
        "            in_channels=c_in, out_channels=c_out, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
        "        ),\n",
        "        'norm': batch_norm(c_out),\n",
        "        'act': relu(),\n",
        "    }\n",
        "\n",
        "\n",
        "def conv_pool_block( c_in, c_out):\n",
        "    return dict(conv_block(c_in, c_out), pool=pool(2))\n",
        "\n",
        "\n",
        "def conv_pool_block_pre(c_in, c_out):\n",
        "    return reorder(conv_pool_block(c_in, c_out), ('conv', 'pool', 'norm', 'act'))\n",
        "\n",
        "\n",
        "def residual(c, conv_block):\n",
        "    return {\n",
        "        'in': (Identity, {}),\n",
        "        'res1': conv_block(c, c),\n",
        "        'res2': conv_block(c, c),\n",
        "        'out': (Identity, {}),\n",
        "        'add': (Add, {}, ['in', 'out']),\n",
        "    }\n",
        "\n",
        "\n",
        "def build_network(channels, extra_layers, res_layers, scale, conv_block=conv_block,\n",
        "                  prep_block=conv_block, conv_pool_block=conv_pool_block, types=None):\n",
        "    net = {\n",
        "        'prep': prep_block(3, channels['prep']),\n",
        "        'layer1': conv_pool_block(channels['prep'], channels['layer1']),\n",
        "        'layer2': conv_pool_block(channels['layer1'], channels['layer2']),\n",
        "        'layer3': conv_pool_block(channels['layer2'], channels['layer3']),\n",
        "        'pool': pool(4),\n",
        "        'classifier': {\n",
        "            'flatten': (Flatten, {}),\n",
        "            'conv': linear(channels['layer3'], 10, bias=False),\n",
        "            'scale': (Mul, {'weight': scale}),\n",
        "        },\n",
        "        'logits': (Identity, {}),\n",
        "    }\n",
        "    for layer in res_layers:\n",
        "        net[layer]['residual'] = residual(channels[layer], conv_block)\n",
        "    for layer in extra_layers:\n",
        "        net[layer]['extra'] = conv_block(channels[layer], channels[layer])\n",
        "    if types: net = map_types(types, net)\n",
        "    return net\n",
        "\n",
        "\n",
        "def label_smoothing_loss(alpha):\n",
        "    return Network({\n",
        "        'logprobs': (LogSoftmax, {'dim': 1}, ['logits']),\n",
        "        'KL': (KLLoss, {}, ['logprobs']),\n",
        "        'xent': (CrossEntropyLoss, {}, ['logprobs', 'target']),\n",
        "        'loss': (AddWeighted, {'wx': 1 - alpha, 'wy': alpha}, ['xent', 'KL']),\n",
        "        'acc': (Correct, {}, ['logits', 'target']),\n",
        "    })\n",
        "\n",
        "\n",
        "def whitening_block(c_in, c_out, eigen_values=None, eigen_vectors=None, eps=1e-2):\n",
        "    filt = nn.Conv2d(3, 27, kernel_size=(3, 3), padding=(1, 1), bias=False)\n",
        "    filt.weight.data = (eigen_vectors / torch.sqrt(eigen_values + eps)[:, None, None, None])\n",
        "    filt.weight.requires_grad = False\n",
        "\n",
        "    return {\n",
        "        'whiten': (identity, {'value': filt}),\n",
        "        'conv': conv(27, c_out, kernel_size=(1, 1), bias=False),\n",
        "        'norm': batch_norm(c_out),\n",
        "        'act': relu(),\n",
        "    }\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv4vfbS5gHm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################\n",
        "# Evaluation functions\n",
        "######################\n",
        "\n",
        "def forward_tta(tta_transforms, model, batch, loss):\n",
        "    \"\"\"\n",
        "    Forward pass with test time augmentation\n",
        "    \"\"\"\n",
        "    if model.training:\n",
        "        model.train(False)\n",
        "    logits = torch.mean(torch.stack(\n",
        "        [model({'input': transform(batch['input'].clone())})['logits'].detach() for transform in tta_transforms],\n",
        "        dim=0), dim=0)\n",
        "    return loss(dict(batch, logits=logits))\n",
        "\n",
        "\n",
        "def eval_on_batches(model, loss, vbatches):\n",
        "    eval_log = []\n",
        "    model.eval()\n",
        "    for tb in vbatches:\n",
        "        out = forward_tta([identity, flip_lr], model, tb, loss)\n",
        "        eval_log.append(('loss', out['loss'].detach()))\n",
        "        eval_log.append(('acc', out['acc'].detach()))\n",
        "    # Average the activations\n",
        "    res = map_values((lambda xs: to_numpy(torch.cat(xs)).astype(np.float)), group_by_key(eval_log))\n",
        "    valid_summary = mean_members(res)\n",
        "    return valid_summary\n",
        "\n",
        "\n",
        "def save_log_to_tsv(log, path):\n",
        "    with open(path, 'w') as f:\n",
        "        tsv_writer = csv.writer(f, delimiter='\\t')\n",
        "        tsv_writer.writerow(['epochs', 'hours', 'top1Accuracy'])\n",
        "        for epoch, l in enumerate(log):\n",
        "            # Save the time in seconds and the accuracy as a percentage\n",
        "            tsv_writer.writerow([epoch, l['time']/3600.0, l['valid']['acc']*100])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x3Up8ZrgUN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################\n",
        "# Compat\n",
        "#####################\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, net, loss=None):\n",
        "        super().__init__()\n",
        "        self.graph = {path: (typ, typ(**params), inputs) for path, (typ, params, inputs) in build_graph(net).items()}\n",
        "        self.loss = loss or identity\n",
        "        for path, (_, node, _) in self.graph.items():\n",
        "            setattr(self, path, node)\n",
        "\n",
        "    def nodes(self):\n",
        "        return (node for _, node, _ in self.graph.values())\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = dict(inputs)\n",
        "        for k, (_, node, ins) in self.graph.items():\n",
        "            outputs[k] = node(*[outputs[x] for x in ins])\n",
        "        return outputs\n",
        "\n",
        "    def half(self):\n",
        "        for node in self.nodes():\n",
        "            if isinstance(node, nn.Module) and not isinstance(node, nn.BatchNorm2d):\n",
        "                node.half()\n",
        "        return self\n",
        "\n",
        "\n",
        "def to_numpy(x):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x.detach().cpu().numpy()\n",
        "    return x\n",
        "\n",
        "\n",
        "def flip_lr(x):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return torch.flip(x, [-1])\n",
        "    return x[..., ::-1].copy()\n",
        "\n",
        "\n",
        "def trainable_params(model):\n",
        "    return {k: p for k, p in model.named_parameters() if p.requires_grad}\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJY5O6Ymg0eo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################\n",
        "# Optimisers\n",
        "#####################\n",
        "\n",
        "\n",
        "def nesterov_update(w, dw, v, lr, weight_decay, momentum):\n",
        "    dw.add_(weight_decay, w).mul_(-lr)\n",
        "    v.mul_(momentum).add_(dw)\n",
        "    w.add_(dw.add_(momentum, v))\n",
        "\n",
        "\n",
        "def zeros_like(weights):\n",
        "    return [torch.zeros_like(w) for w in weights]\n",
        "\n",
        "\n",
        "class SGDOpt(object):\n",
        "    \"\"\"\n",
        "    A class to hold the optimizer state\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 weight_param_schedule,\n",
        "                 bias_param_schedule,\n",
        "                 weight_params,\n",
        "                 bias_params):\n",
        "\n",
        "        self.weights = weight_params\n",
        "        self.bias = bias_params\n",
        "        self._w_param_schedule = weight_param_schedule\n",
        "        self._b_param_schedule = bias_param_schedule\n",
        "        # Internal optimizer state initialization\n",
        "        self.opt_state = zeros_like(self.weights)\n",
        "        self.bias_opt_state = zeros_like(self.bias)\n",
        "        self.step_number = 0\n",
        "        self.update = nesterov_update\n",
        "        self.last_step_b_parameters = {}\n",
        "        self.last_step_w_parameters = {}\n",
        "\n",
        "    def step(self):\n",
        "        self.step_number += 1\n",
        "        # The weights\n",
        "        param_values = {k: f(self.step_number) for k, f in self._w_param_schedule.items()}\n",
        "        self.last_step_w_parameters = param_values\n",
        "        for w, v in zip(self.weights, self.opt_state):\n",
        "            if w.requires_grad:\n",
        "                self.update(w.data, w.grad.data, v, **param_values)\n",
        "\n",
        "        param_values = {k: f(self.step_number) for k, f in self._b_param_schedule.items()}\n",
        "        self.last_step_b_parameters = param_values\n",
        "        for w, v in zip(self.bias, self.bias_opt_state):\n",
        "            if w.requires_grad:\n",
        "                self.update(w.data, w.grad.data, v, **param_values)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDtS6Jq5g5ZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################\n",
        "# Hyperparameter Schedules\n",
        "####################\n",
        "\n",
        "\n",
        "class PiecewiseLinear(namedtuple('PiecewiseLinear', ('knots', 'vals'))):\n",
        "    def __call__(self, t):\n",
        "        return np.interp([t], self.knots, self.vals)[0]\n",
        "\n",
        "\n",
        "class Const(namedtuple('Const', ['val'])):\n",
        "    def __call__(self, x):\n",
        "        return self.val\n",
        "\n",
        "\n",
        "def lr_schedule(knots, vals, batch_size, batch_count):\n",
        "    return PiecewiseLinear(np.array(knots) * batch_count, np.array(vals) / batch_size)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BSbJGlBg-XT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#####################\n",
        "# DATA\n",
        "#####################\n",
        "\n",
        "@cache(None)\n",
        "def cifar10(root='./data'):\n",
        "    download = lambda train: torchvision.datasets.CIFAR10(root=root, train=train, download=True)\n",
        "    return {k: {'data': torch.tensor(v.data), 'targets': torch.tensor(v.targets)}\n",
        "            for k, v in [('train', download(True)), ('valid', download(False))]}\n",
        "\n",
        "\n",
        "def normalise(data, mean, std):\n",
        "    return (data - mean) / std\n",
        "\n",
        "\n",
        "def pad(data, border):\n",
        "    return nn.ReflectionPad2d(border)(data)\n",
        "\n",
        "\n",
        "def transpose(x, source='NHWC', target='NCHW'):\n",
        "    return x.permute([source.index(d) for d in target])\n",
        "\n",
        "\n",
        "def preprocess(dataset, transforms):\n",
        "    dataset = copy.copy(dataset)\n",
        "    for transform in reversed(transforms):\n",
        "        dataset['data'] = transform(dataset['data'])\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def compute_patch_whitening_statistics(train_set):\n",
        "\n",
        "    def cov(X):\n",
        "        X = X/np.sqrt(X.size(0) - 1)\n",
        "        return X.t() @ X\n",
        "\n",
        "    def patches(data, patch_size=(3, 3), dtype=torch.float32):\n",
        "        h, w = patch_size\n",
        "        c = data.size(1)\n",
        "        return data.unfold(2, h, 1).unfold(3, w, 1).transpose(1, 3).reshape(-1, c, h, w).to(dtype)\n",
        "\n",
        "    def eigens(patches):\n",
        "        n, c, h, w = patches.shape\n",
        "        covariance = cov(patches.reshape(n, c*h*w))\n",
        "        eigen_values, eigen_vectors = torch.symeig(covariance, eigenvectors=True)\n",
        "        return eigen_values.flip(0), eigen_vectors.t().reshape(c*h*w, c, h, w).flip(0)\n",
        "\n",
        "    eigen_values, eigen_vectors = eigens(\n",
        "        patches(train_set['data'][:10000, :, 4:-4, 4:-4])\n",
        "    ) # center crop to remove padding\n",
        "    return eigen_values, eigen_vectors\n",
        "\n",
        "\n",
        "def chunks(data, splits):\n",
        "    return (data[start:end] for (start, end) in zip(splits, splits[1:]))\n",
        "\n",
        "\n",
        "def even_splits(N, num_chunks):\n",
        "    return np.cumsum(\n",
        "        [0] + [(N // num_chunks) + 1] * (N % num_chunks) + [N // num_chunks] * (num_chunks - (N % num_chunks))\n",
        "    )\n",
        "\n",
        "\n",
        "def shuffled(xs, inplace=False):\n",
        "    xs = xs if inplace else copy.copy(xs)\n",
        "    np.random.shuffle(xs)\n",
        "    return xs\n",
        "\n",
        "\n",
        "def transformed(data, targets, transform, max_options=None, unshuffle=False, device=None):\n",
        "    i = torch.randperm(len(data), device=device)\n",
        "    data = data[i]\n",
        "    options = shuffled(transform.options(data.shape), inplace=True)[:max_options]\n",
        "    data = torch.cat([transform.apply(x, **choice) for choice, x in\n",
        "                      zip(options, chunks(data, even_splits(len(data), len(options))))])\n",
        "    return (data[torch.argsort(i)], targets) if unshuffle else (data, targets[i])\n",
        "\n",
        "\n",
        "class Batches():\n",
        "    \"\"\"\n",
        "    An iterable that returns batches of data\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size, transforms=(), dataset=None, shuffle=True, drop_last=False, max_options=None,\n",
        "                 device=None):\n",
        "        self.dataset, self.transforms, self.shuffle, self.max_options = dataset, transforms, shuffle, max_options\n",
        "        self.device = device\n",
        "        # Shard data per worker\n",
        "        N = len(dataset['data'])\n",
        "        self.splits = list(range(0, N + 1, batch_size))\n",
        "        if not drop_last and self.splits[-1] != N:\n",
        "            self.splits.append(N)\n",
        "\n",
        "    def __iter__(self):\n",
        "        data, targets = self.dataset['data'], self.dataset['targets']\n",
        "        for transform in self.transforms:\n",
        "            data, targets = transformed(data, targets, transform, max_options=self.max_options,\n",
        "                                        unshuffle=not self.shuffle, device=self.device)\n",
        "\n",
        "        if self.shuffle:\n",
        "            i = torch.randperm(len(data), device=self.device)\n",
        "            data, targets = data[i], targets[i]\n",
        "\n",
        "        return ({'input': x.clone(), 'target': y} for (x, y) in\n",
        "                zip(chunks(data, self.splits), chunks(targets, self.splits)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.splits) - 1\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvRxGIMmhCvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################\n",
        "# Augmentations\n",
        "#####################\n",
        "\n",
        "class Crop(namedtuple('Crop', ('h', 'w'))):\n",
        "    def apply(self, x, x0, y0):\n",
        "        return x[..., y0:y0 + self.h, x0:x0 + self.w]\n",
        "\n",
        "    def options(self, shape):\n",
        "        *_, H, W = shape\n",
        "        return [{'x0': x0, 'y0': y0} for x0 in range(W + 1 - self.w) for y0 in range(H + 1 - self.h)]\n",
        "\n",
        "\n",
        "class FlipLR(namedtuple('FlipLR', ())):\n",
        "    def apply(self, x, choice):\n",
        "        return flip_lr(x) if choice else x\n",
        "\n",
        "    def options(self, shape):\n",
        "        return [{'choice': b} for b in [True, False]]\n",
        "\n",
        "\n",
        "class Cutout(namedtuple('Cutout', ('h', 'w'))):\n",
        "    def apply(self, x, x0, y0):\n",
        "        x[..., y0:y0 + self.h, x0:x0 + self.w] = 0.0\n",
        "        return x\n",
        "\n",
        "    def options(self, shape):\n",
        "        *_, H, W = shape\n",
        "        return [{'x0': x0, 'y0': y0} for x0 in range(W + 1 - self.w) for y0 in range(H + 1 - self.h)]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0sy-LVRhFSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#####################\n",
        "# Timing\n",
        "#####################\n",
        "\n",
        "\n",
        "class Timer(object):\n",
        "    def __init__(self, synch=None):\n",
        "        self.synch = synch or (lambda: None)\n",
        "        self.synch()\n",
        "        self.times = [time.perf_counter()]\n",
        "        self.total_time = 0.0\n",
        "\n",
        "    def __call__(self, update_total=True):\n",
        "        self.synch()\n",
        "        self.times.append(time.perf_counter())\n",
        "        delta_t = self.times[-1] - self.times[-2]\n",
        "        if update_total:\n",
        "            self.total_time += delta_t\n",
        "        return delta_t"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}